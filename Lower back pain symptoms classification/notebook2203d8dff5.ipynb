{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4328de1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-02T11:29:04.488867Z",
     "iopub.status.busy": "2022-07-02T11:29:04.488265Z",
     "iopub.status.idle": "2022-07-02T11:29:07.515850Z",
     "shell.execute_reply": "2022-07-02T11:29:07.514757Z"
    },
    "papermill": {
     "duration": 3.038682,
     "end_time": "2022-07-02T11:29:07.518963",
     "exception": false,
     "start_time": "2022-07-02T11:29:04.480281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR,ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b7b20bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:29:07.529765Z",
     "iopub.status.busy": "2022-07-02T11:29:07.529177Z",
     "iopub.status.idle": "2022-07-02T11:29:07.560228Z",
     "shell.execute_reply": "2022-07-02T11:29:07.558945Z"
    },
    "papermill": {
     "duration": 0.03919,
     "end_time": "2022-07-02T11:29:07.562906",
     "exception": false,
     "start_time": "2022-07-02T11:29:07.523716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\n",
    "\n",
    "# Function to get the learning rate\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# Function to compute the loss value per batch of data\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    \n",
    "    loss = loss_func(output, target) # get loss\n",
    "    \n",
    "    if(opt is not None):\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred)) \n",
    "    acc = accuracy_score(y_test,y_pred_tag.detach().numpy())\n",
    "    acc = round(100*acc)\n",
    "    return acc\n",
    "\n",
    "def recall(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred)) \n",
    "    get_recall = recall_score(y_test,y_pred_tag.detach().numpy())\n",
    "    get_recall = round(100*get_recall)\n",
    "    return get_recall\n",
    "\n",
    "def precision(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred)) \n",
    "    get_precision = precision_score(y_test,y_pred_tag.detach().numpy())\n",
    "    get_precision = round(100*get_precision)\n",
    "    return get_precision\n",
    "\n",
    "def f1(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    get_f1 = f1_score(y_test,y_pred_tag.detach().numpy())\n",
    "    get_f1 = round(100*get_f1)\n",
    "    return get_f1\n",
    "\n",
    "# Compute the loss value & performance metric for the entire dataset (epoch)\n",
    "def loss_epoch(model,loss_func,dataset_dl,eval_funcs,opt=None):\n",
    "    \n",
    "    run_loss=0.0\n",
    "    \n",
    "    t_metric = {}; metric = {}\n",
    "    for i in eval_funcs:\n",
    "        t_metric[i] = 0.0\n",
    "        \n",
    "    # internal loop over dataset\n",
    "    for xb, yb in dataset_dl:\n",
    "        xb=xb.to(device)\n",
    "        yb=yb.to(device)\n",
    "        y_pred  = model(xb)\n",
    "        \n",
    "        loss = loss_batch(loss_func,y_pred, yb.unsqueeze(1),opt=opt)\n",
    "        \n",
    "        for feval in eval_funcs:\n",
    "            if(feval == 'accuracy'):\n",
    "                t_metric[feval] += accuracy(y_pred, yb.unsqueeze(1))\n",
    "            if(feval == 'f1'):\n",
    "                t_metric[feval] += f1(y_pred,yb.unsqueeze(1))\n",
    "            if(feval == 'recall'):\n",
    "                t_metric[feval] += recall(y_pred,yb.unsqueeze(1))\n",
    "        \n",
    "        run_loss += loss.item()\n",
    "    loss=run_loss/len(dataset_dl)  # average loss value\n",
    "    \n",
    "    for feval in eval_funcs:\n",
    "        temp = t_metric[feval]/len(dataset_dl)\n",
    "        metric[feval] = temp  # average metric value\n",
    "        \n",
    "    \n",
    "    return loss, metric\n",
    "\n",
    "# Training Function\n",
    "def train_val(model, params,verbose=False):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    epochs=params[\"epochs\"]\n",
    "    loss_func=params[\"f_loss\"]\n",
    "    opt=params[\"optimiser\"]\n",
    "    train_dl=params[\"train\"]\n",
    "    val_dl=params[\"val\"]\n",
    "    lr_scheduler=params[\"lr_change\"]\n",
    "    weight_path=params[\"weight_path\"]\n",
    "    eval_funcs = params['eval_func'] # list of evaluation functions\n",
    "    write_metric = params['write_metric']\n",
    "    \n",
    "    loss_history={\"train\": [],\"val\": []} # history of loss values in each epoch\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # a deep copy of weights for the best performing model\n",
    "    best_loss=float('inf') # initialize best loss to a large value\n",
    "    \n",
    "    tr_dict_eval = {}; te_dict_eval = {}\n",
    "    for evals in eval_funcs:\n",
    "        tr_dict_eval[evals] = []\n",
    "        te_dict_eval[evals] = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        current_lr=get_lr(opt)\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model,loss_func,train_dl,eval_funcs,opt)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model,loss_func,val_dl,eval_funcs)\n",
    "        \n",
    "        if(val_loss < best_loss):\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), weight_path)\n",
    "                \n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        \n",
    "        for evals in eval_funcs:\n",
    "            tr_dict_eval[evals].append(train_metric[evals])\n",
    "            te_dict_eval[evals].append(val_metric[evals])\n",
    "        \n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            if(verbose):\n",
    "                print(\"Saving best model weights!\")\n",
    "            model.load_state_dict(best_model_wts) \n",
    "\n",
    "        if(verbose):\n",
    "            print(f\"epoch: {epoch+1+0:03} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f} | train-{write_metric}: {train_metric[write_metric]:.3f} val-{write_metric}: {val_metric[write_metric]:.3f}\")\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f'Total Time: {t1-t0:.3f}')\n",
    "        \n",
    "    return model, loss_history, {'train':tr_dict_eval,'val':te_dict_eval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00c69de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:29:07.572830Z",
     "iopub.status.busy": "2022-07-02T11:29:07.572438Z",
     "iopub.status.idle": "2022-07-02T11:29:07.617266Z",
     "shell.execute_reply": "2022-07-02T11:29:07.616192Z"
    },
    "papermill": {
     "duration": 0.05317,
     "end_time": "2022-07-02T11:29:07.620266",
     "exception": false,
     "start_time": "2022-07-02T11:29:07.567096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    210\n",
       "0    100\n",
       "Name: Class_att, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../input/lower-back-pain-symptoms-dataset/Dataset_spine.csv')\n",
    "df.drop([df.columns[-1]],axis=1,inplace=True)\n",
    "\n",
    "df['Class_att'] = df['Class_att'].astype('category')\n",
    "encode_map = {\n",
    "    'Abnormal': 1,\n",
    "    'Normal': 0\n",
    "}\n",
    "\n",
    "df['Class_att'].replace(encode_map, inplace=True)\n",
    "df['Class_att'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb7ee08c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:29:07.631328Z",
     "iopub.status.busy": "2022-07-02T11:29:07.630237Z",
     "iopub.status.idle": "2022-07-02T11:29:07.657083Z",
     "shell.execute_reply": "2022-07-02T11:29:07.655586Z"
    },
    "papermill": {
     "duration": 0.034912,
     "end_time": "2022-07-02T11:29:07.659502",
     "exception": false,
     "start_time": "2022-07-02T11:29:07.624590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (248, 12)\n",
      "X_val: (62, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['Class_att']\n",
    "X = df.drop(['Class_att'],axis=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "                                                  test_size=0.20,\n",
    "                                                  random_state=13,\n",
    "                                                  shuffle=True)\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_val: {X_val.shape}')\n",
    "\n",
    "X_train = X_train.values\n",
    "X_val = X_val.values\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train = sc.transform(X_train)\n",
    "X_val = sc.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5363730",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:29:07.670797Z",
     "iopub.status.busy": "2022-07-02T11:29:07.669633Z",
     "iopub.status.idle": "2022-07-02T11:29:10.002374Z",
     "shell.execute_reply": "2022-07-02T11:29:10.001307Z"
    },
    "papermill": {
     "duration": 2.340916,
     "end_time": "2022-07-02T11:29:10.004795",
     "exception": false,
     "start_time": "2022-07-02T11:29:07.663879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"62e3d4d9-2c0a-43f0-a8af-f0f16dd67f8e\" class=\"plotly-graph-div\" style=\"height:525px; width:1200px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"62e3d4d9-2c0a-43f0-a8af-f0f16dd67f8e\")) {                    Plotly.newPlot(                        \"62e3d4d9-2c0a-43f0-a8af-f0f16dd67f8e\",                        [{\"hovertemplate\":\"x=%{x}<br>y=%{y}<br>color=%{marker.color}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":[0,1,0,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,0,1,0,1,0,1,1,1,1,1,0,1,0,0,1,1,1,1,1,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,1,1,1,0,0,1,1,1,0,1,1,1,0,1,1,1,0,1,0,1,1,0,0,1,0,0,0,0,1,1,1,1,0,1,1,1,0,1,1,1,1,0,1,0,1,1,0,0,1,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,1,1,1,0,0,1,1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,0,0,1,1,0,1,0,1,0,1,1,1,0,1,0,0,0,1,1,1,1,0,1,1,1,0,0,1,0,0,1,1,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,0,1,0,0,1,0,1,1,1,0,0,0,1,1,0,1,0,1,1,1,1,1,0,1,1,1,1,1,1,0,0,0,1,0,0,1,0,0,1,1,1],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1.5},\"size\":7},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[-1.226860481988446,-1.7678416025465358,-1.3224238786079405,0.3238640931004509,0.9915548942432846,0.9906870339546509,-0.5062803132467376,1.6320749064175673,2.064335926139832,1.4300527315251095,-0.709185665938512,-0.8107385088408883,1.6672879971059626,0.4917211913515222,-0.863685877250807,0.6320625717241294,-0.2099862333536006,-0.5894064498696068,-0.3704544021391108,0.7236340079335095,-1.0060186990892093,0.35498287968430503,-0.6721072162717668,1.4714433624048866,1.072060005285988,0.18338171396695596,-1.1194097124915428,1.2242475856030566,1.3313438728664797,-0.9231217946474692,0.23904549667681926,-1.519205925106518,0.2368689644134636,1.0249952206990045,1.208706321419807,-0.37526862395854815,-1.2656796198197235,1.6902558565335155,0.20735324925783813,-1.1744884848433825,0.8844682719214034,0.421198425767715,-0.21612340132412558,-0.8147327057227051,-1.5979619131907032,0.030027696458237683,-1.6538120661925775,2.0700714597453507,1.2227372546113295,-1.7016888304111764,-0.6189593796323767,1.4750940021029335,0.38549585854443896,3.271023975835924,0.1636123216040938,-0.08308917556523568,-0.7039470614236955,0.4442361739079054,-1.2093425287106618,-1.1490496079293664,1.1861275551967931,0.07624372094163023,-0.7174173428236944,-1.4539889075354913,0.2802798753184966,1.2637309208696161,-0.43357929295801045,-0.5246620762556594,-0.0268756302783153,0.8303746333377151,0.18573988450772808,-1.7330636862491255,1.022912172012866,-0.9218316964909278,-0.4595938472051134,1.7384161345159328,0.9343282337372915,1.6117832130238001,-0.12576508388004418,-1.4313642598634555,-0.2584523357702625,-0.9288994916485812,0.25738260897173876,0.587914462879581,-1.414900079532627,0.23608703116527044,0.022527172518135467,0.841380030952602,-0.3126415945341574,-0.5238576866462125,-1.5540692849623072,-0.8991754174602522,0.850321959436214,-0.9354669927273758,-0.9414850978060361,-0.9254585016394847,1.5155681201820075,0.2682820106346277,0.8583850085873983,-0.799236851843138,-0.9393107802782503,1.4922168659411734,1.5574148520603304,-1.087557857833176,-0.35837263774966904,4.088230853368539,-1.4972090363012627,1.4528771371838303,2.1392209312477375,0.19835339398352247,0.08765675091891734,0.17214049812306625,0.3239034090628045,-0.24747078210427417,-0.09653327843831089,0.06867357029739435,-0.1819314271010829,-1.357192414295909,-0.15981317548625396,-0.40005194208093525,-1.4415927540884927,0.9983432220823742,0.5045878098552724,-0.3127247164386965,0.01040869345257567,0.596660150517585,-0.9665271305284793,0.7468354097583406,-0.7306390750836682,-1.0853390893288126,1.9933836610577753,1.1672196902712513,-0.8542277890746602,0.6429230234141236,-0.1728624632650141,-0.6145233870639202,-0.5574707583642101,-0.6661740871309252,-0.8137826446715409,0.6016416030839683,-0.9788184030282848,0.8926498340990694,1.1650662047694798,-0.5481212791755565,-0.08578157555200812,-0.3108725859525437,-0.2136591431871253,-0.9004987019900371,0.5512477672157576,-1.041403998686149,-1.1564441500165834,1.4543354236520334,-0.2178136745875414,-1.1097321786806515,0.2799386204022863,1.0084731652742172,-0.9924098140150399,-0.7393234962037218,0.21834649520023644,0.2085301991001609,0.9441280309385588,1.5409245031894485,-0.0631604920180908,0.7108797575930923,-0.337148919594258,0.4271032996998699,-1.2769964838636616,0.7285186747930621,1.199104127978344,1.3602033195751524,-0.8910228589191546,0.38986746810364464,-0.7591845680448789,0.07072144346051741,-0.5382875830084066,0.5319040816899131,1.0667448525777825,1.2255270882766396,1.3020396761196593,0.11150363830704016,-0.4722062093656371,0.6291714870456031,-0.6185922264091015,0.4139828619253766,0.21658107308033608,1.4944000471109464,-0.5923197829474799,-1.5509676206408753,-0.9587344588806104,-1.6894574685646806,-0.6118202153552845,-0.3853388233365783,-0.2911995316496945,-0.7864350328599168,0.6461512930650841,-0.37016593958684435,-0.22298485949677643,-0.7127991444885485,-2.0029451775035163,-0.011665513122620152,-0.35939996898626914,1.7190457746443781,1.1299396714693422,0.8085129914429563,-0.5978003470296702,-0.3314722104229315,-0.5532563868060462,-1.2992427853795763,-0.869386635395624,-0.32253082181783144,0.8655251163300943,-0.30324758506141936,-0.24314965441696998,-0.9707526321609466,0.6938691655742512,-0.19100465709357461,-1.5034840624553907,-0.6607075457909293,-1.0413063417716646,0.3185574239456604,0.5589072451363714,0.42857952787604875,-1.4167885581179762,-0.7004803904688494,0.49811918643910913,-0.3231620196931086,-1.3123243178283648,0.1663025180170929,-1.7042516084458317,-0.596662625617712,1.4885701070238115,-1.244587432871232,0.5862117547865028,1.4600072028337285,1.116586230588619,1.8671663878608984,-1.1686669208039315,-0.8806046822856213,-0.9873080266993612,0.6129675222828366,-0.7248815674240311,0.0927531006769185,-0.17364271284420277,0.16352317702871363,0.31541187261214715,0.21103022844767574,0.7037561787098179,0.3985885381039589],\"xaxis\":\"x\",\"y\":[-0.8066565557370236,-0.9664621394467292,-0.5403139167535327,-0.07033128719334726,1.3497111244249878,0.9235387039703064,-0.913193611543494,1.5784137128776117,2.2913149261270886,0.7104037192144184,-0.6622498226257122,-1.2308215810153624,-0.2439138455767276,0.5791080141541789,-1.1636451523871456,1.725574385734013,-0.4460440814242891,-0.8543847673453834,-1.2210572675513136,-0.007628637721180335,-0.6283461358424597,-0.2473370132857389,-0.007628637721180335,2.588904195349236,0.41851958550470153,0.8179645662968134,-0.49410924736738315,1.41819707245132,0.32532854493523955,-0.007628637721180335,-0.06308308334711758,-1.0521807913087984,0.8820823831711718,0.6544694360159812,0.506010541483548,-1.0314998276239815,-1.1262677231564349,0.8990687712031433,0.7040668216244809,-1.4392230093217482,0.4875664447376314,0.4262243650899807,0.02315238936100362,-0.5030259472212679,-1.7617040186819464,-0.47643655188338857,-0.8600210479586471,0.3652510576014663,2.111632851227351,-1.0430092313792723,-1.1084493957786354,1.0400919156543613,1.062265045865768,1.3134308542790538,-0.6676727334870329,3.920519682469384,-0.8423924359102893,-0.4733659323379699,-0.823976447901177,-1.125117521402314,0.7666836182102262,0.03989906475592835,0.33033456298005764,-1.9471163776988236,0.29791096781349025,0.9810586990898692,-0.27460773919707926,0.2587140017949958,0.37624895457904833,1.4184770864585228,-0.21341531481514672,-0.8650641131992515,2.2239380197960514,-1.3507457758950678,-0.8667083230196648,2.046590184573481,1.6416450221261574,2.2201898906286788,1.465237543151571,-0.46268798425027413,-0.23718705592171116,-1.0600285643976959,0.8250738618028303,1.2905291336562297,-0.9863145990730824,-0.4389703485201861,0.6601063276192601,0.3449335061965076,-0.02885852080159347,-1.0059566075301367,-1.4192446266242287,-1.297761724303618,-1.0463649313015821,-0.8546641240189516,-0.32136220371342405,-1.1010102418384866,-0.22755281834326427,0.35672774848957317,0.5619855197065617,-0.6468509725600031,-0.7533880278337883,1.4173398278913114,0.9097110677304834,-1.1730062507946013,-0.48704538885029747,-0.2002445951170766,-1.2059371391983202,0.46432373522177006,2.0278416251641103,-0.14882953871732613,-0.2792981300276801,0.5783251692144072,-0.43377686094706225,0.5397201124242681,-0.034262901672797955,-0.694959416518592,-0.49545395116086366,-1.4991474184790816,-0.06569887489388733,-0.7978175027528366,-1.674933560559758,-0.1261226461372523,-0.10925467723892165,-0.5296602111728855,0.40862418991255933,0.482441818988584,-0.5035152724512723,0.4721647847570758,0.09890841808529013,-1.710576262358645,0.8289920171863613,1.755645277818957,-0.5095328891386635,-0.44993506399905214,-0.27397127723735654,0.0456398901820549,-0.8066565557370236,0.178811209940143,-1.0657251939915917,0.5472771863719508,-0.27397127723735654,1.9952680114404646,0.013861950057755654,-0.27397127723735654,0.06369476924245554,0.0456398901820549,-0.5403139167535327,-0.4001690062603731,0.033206735044297817,-0.5403139167535327,-0.9664621394467292,1.66245660397019,-0.38095863108686295,-1.1419184774271718,-0.06369275123730526,0.5906995782215367,-1.2825851350398438,-0.8237032618538809,0.5849164218943415,0.7381307529241133,2.1915552288545435,0.4792995916264757,-0.47643655188338857,0.4115420683464385,-0.27397127723735654,2.351264870085957,-0.9071841721078215,0.8446678087305838,0.46636102842476934,1.3999012763524565,-0.40271187052901686,-0.1525937832124458,-0.8833498133140214,-0.3181656536392452,-0.8625230467434245,-0.41719766340192116,1.4676039618137584,1.3713348355820072,1.3269911517578876,0.3119825296982311,1.3824119348703017,0.5708201210398718,-1.2691655564984865,-0.06089716562441557,-0.6374197988714054,1.6267018656726762,-0.5403139167535327,-0.8257675488833951,-0.762308018617327,-1.4839755749084589,-1.2860733068661407,-0.5624577961508972,-0.9664621394467292,-0.7533880278337883,1.8034791608695453,-0.4816919976793723,-0.4562643612354282,-0.8831137911205876,-2.031832697511434,0.3140528216888961,-0.8956284780017829,1.05792425372037,0.9858817120417049,0.2789295290538321,-1.0640254368156514,-1.2136165933273788,0.22142603226273141,-0.0904572173209355,-0.380508333043827,-0.2207027493341213,0.08452973650292306,0.2054454738917606,0.558989733919883,-0.007628637721180335,1.4771791544251365,0.2591936939444143,-0.48704538885029747,-0.014366042195752217,-1.4286430391027953,0.2315009997912324,-0.19406848538250365,0.3120586552187724,-1.2328047789629055,-0.2207027493341213,0.8476515073555959,-0.5935824446567679,-1.461312143054931,0.09890841808529013,-1.951929905656581,-0.9420896300229337,1.3896783331149223,-1.4450597154346743,0.850974967566763,2.1089852354639564,-0.6044120269359929,1.3462955911613963,-0.7551061195043479,-0.5625201592145697,-0.8582085378223789,0.37490122832929473,-0.8599250836402588,0.6315936971176425,0.6315936971176425,-0.028668818789283172,0.5560684564212378,0.12833574698305705,2.0698439505049944,0.7461425179703279],\"yaxis\":\"y\",\"type\":\"scatter\",\"opacity\":1.0},{\"hovertemplate\":\"x=%{x}<br>y=%{y}<br>color=%{marker.color}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":[0,1,1,0,0,1,1,1,0,1,0,0,1,0,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,0,1,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,0,1,1,0,1,1,1,0,1,1],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1.5},\"size\":7},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.5378850620432692,-0.6996751255395082,1.1570565037222527,-1.0177627022049982,-1.1649030869695505,-1.0010115191674984,1.0681957317248156,0.21488698130731598,-0.9329066274033279,-0.6654311208920539,-0.5619715228424346,0.36801192113767767,1.5660614176510879,-0.8228214052715622,0.3109589170491005,1.38859744681563,0.9448314714907797,0.5603371800519542,1.7293628007208706,-0.10110994960031079,1.257951032631454,0.8336594586407009,-1.2428066262698658,-0.7342155558004125,-1.399352537132124,-0.8116615471807743,-0.7156547957477953,0.813752550585174,0.7868015990891986,-0.5480824419955614,-0.39187198661461775,-1.0924365725174483,1.218710991060778,1.3917149814504424,-1.120354930497602,-1.1453035313395734,-0.038021268826798314,0.8505286389121545,-1.3838513050286443,-1.2677154934632793,-0.2779017058927065,-0.5535948207248527,-0.5118796723376581,-1.3039447112936313,-1.0016735717753076,-0.37196379965210513,0.8357442855773897,1.3782132502178754,-0.873909863872122,0.4124223986022433,1.7783215813458504,1.8344114186708373,-0.8616478264693139,0.6951759341688011,3.401522669507383,-0.03034901123573148,0.5475586827630097,0.9051030801392894,-0.18843783437605352,0.5147445630967824,1.226026695611233,-1.0704378373159333],\"xaxis\":\"x2\",\"y\":[1.269024155917166,-0.8502305748532303,0.594936153688204,0.3119825296982311,-1.127185641635096,-0.913193611543494,1.285594813984599,0.5316797251326403,-0.27918646481069226,-0.6327990763503937,-0.913193611543494,-1.0875163936521,-0.23165760054699,-0.8066565557370236,-0.3392161725305582,1.8643079875971424,0.26936770737564303,0.3119825296982311,1.6506596375246072,-0.5686604852828385,0.32438453819503793,-1.3020538652371112,-1.3429620518187508,-0.7001195004632385,-1.6993006163516666,-0.21537589654379768,0.26175889370680777,1.272528919824534,0.5783251692144072,-1.131779820981339,-1.0144038145596408,-0.8582218309835175,1.8546711398608282,0.525056641311172,-0.9947262727983635,-0.11416569352765081,-0.29739600637843383,1.6295987200839626,-0.543043880334213,-0.6468509725600031,-0.43377686094706225,-0.5121381217978321,-0.913193611543494,-1.3799947750785468,-1.236099476299746,-0.7001195004632385,0.7883150895263907,1.3299759477145752,-1.2307619863170853,-0.023775030607848534,0.9412938452856772,1.7300052331199987,-0.9784932109514262,-0.06089716562441557,-0.0694989914810675,0.1704739012105561,1.187637089268132,0.6395139164698591,-0.1568869122771319,0.18254761949869217,0.4551153779258536,0.8393409559402598],\"yaxis\":\"y2\",\"type\":\"scatter\",\"opacity\":1.0}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Train\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Validation\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"font\":{\"family\":\"sans-serif\",\"size\":12},\"title\":{\"text\":\"Train / Validation Data Splitting\"},\"width\":1200,\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(176, 242, 188)\"],[0.16666666666666666,\"rgb(137, 232, 172)\"],[0.3333333333333333,\"rgb(103, 219, 165)\"],[0.5,\"rgb(76, 200, 163)\"],[0.6666666666666666,\"rgb(56, 178, 163)\"],[0.8333333333333334,\"rgb(44, 152, 160)\"],[1.0,\"rgb(37, 125, 152)\"]],\"showscale\":false}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('62e3d4d9-2c0a-43f0-a8af-f0f16dd67f8e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2,subplot_titles=['Train','Validation'])\n",
    "itrace = px.scatter(x=X_train[:,0],\n",
    "                    y=X_train[:,2],\n",
    "                    color=y_train,\n",
    "                   )['data'][0]\n",
    "trace = px.scatter(x=X_val[:,0],\n",
    "                   y=X_val[:,2],\n",
    "                   color=y_val)['data'][0]\n",
    "\n",
    "fig.add_trace(itrace, row=1, col=1)\n",
    "fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_white',\n",
    "                  title='Train / Validation Data Splitting',\n",
    "                  font=dict(family='sans-serif',size=12),\n",
    "                  width=1200)\n",
    "\n",
    "fig.update_traces({'marker_line_width':1.5, \n",
    "                   'marker_line_color':\"black\",\n",
    "                   'marker_size':7,\n",
    "                   'opacity':1.0,\n",
    "                  })\n",
    "\n",
    "fig.update_coloraxes(colorscale=\"tealgrn\")\n",
    "fig.update_layout(coloraxis_showscale=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2871ba47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:29:10.017091Z",
     "iopub.status.busy": "2022-07-02T11:29:10.016194Z",
     "iopub.status.idle": "2022-07-02T11:29:10.023157Z",
     "shell.execute_reply": "2022-07-02T11:29:10.022421Z"
    },
    "papermill": {
     "duration": 0.015211,
     "end_time": "2022-07-02T11:29:10.025043",
     "exception": false,
     "start_time": "2022-07-02T11:29:10.009832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "train_dataset = TensorDataset(X_train_tensor,\n",
    "                              y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor,\n",
    "                            y_val_tensor)\n",
    "\n",
    "# Builds a loader of each set\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ddd13d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:29:10.036090Z",
     "iopub.status.busy": "2022-07-02T11:29:10.035503Z",
     "iopub.status.idle": "2022-07-02T11:29:10.048894Z",
     "shell.execute_reply": "2022-07-02T11:29:10.047559Z"
    },
    "papermill": {
     "duration": 0.021412,
     "end_time": "2022-07-02T11:29:10.051246",
     "exception": false,
     "start_time": "2022-07-02T11:29:10.029834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 12]) tensor([1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.shape,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c95167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:29:10.062859Z",
     "iopub.status.busy": "2022-07-02T11:29:10.062011Z",
     "iopub.status.idle": "2022-07-02T11:29:27.154083Z",
     "shell.execute_reply": "2022-07-02T11:29:27.152945Z"
    },
    "papermill": {
     "duration": 17.100391,
     "end_time": "2022-07-02T11:29:27.156455",
     "exception": false,
     "start_time": "2022-07-02T11:29:10.056064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 001 | train loss: 0.801 | val loss: 0.697 | train-f1: 47.438 val-f1: 66.500\n",
      "epoch: 002 | train loss: 0.763 | val loss: 0.653 | train-f1: 50.062 val-f1: 71.500\n",
      "epoch: 003 | train loss: 0.720 | val loss: 0.622 | train-f1: 60.250 val-f1: 70.000\n",
      "epoch: 004 | train loss: 0.683 | val loss: 0.595 | train-f1: 60.000 val-f1: 72.250\n",
      "epoch: 005 | train loss: 0.673 | val loss: 0.574 | train-f1: 61.812 val-f1: 76.750\n",
      "epoch: 006 | train loss: 0.654 | val loss: 0.554 | train-f1: 61.438 val-f1: 82.750\n",
      "epoch: 007 | train loss: 0.623 | val loss: 0.541 | train-f1: 72.875 val-f1: 81.500\n",
      "epoch: 008 | train loss: 0.615 | val loss: 0.531 | train-f1: 71.688 val-f1: 81.500\n",
      "epoch: 009 | train loss: 0.615 | val loss: 0.521 | train-f1: 74.250 val-f1: 85.500\n",
      "epoch: 010 | train loss: 0.602 | val loss: 0.516 | train-f1: 73.438 val-f1: 86.750\n",
      "epoch: 011 | train loss: 0.593 | val loss: 0.504 | train-f1: 75.750 val-f1: 85.000\n",
      "epoch: 012 | train loss: 0.565 | val loss: 0.500 | train-f1: 76.750 val-f1: 85.000\n",
      "epoch: 013 | train loss: 0.582 | val loss: 0.494 | train-f1: 76.500 val-f1: 84.000\n",
      "epoch: 014 | train loss: 0.543 | val loss: 0.485 | train-f1: 79.688 val-f1: 84.000\n",
      "epoch: 015 | train loss: 0.562 | val loss: 0.485 | train-f1: 78.500 val-f1: 83.250\n",
      "epoch: 016 | train loss: 0.533 | val loss: 0.479 | train-f1: 78.250 val-f1: 82.250\n",
      "epoch: 017 | train loss: 0.533 | val loss: 0.481 | train-f1: 81.062 val-f1: 85.000\n",
      "epoch: 018 | train loss: 0.533 | val loss: 0.471 | train-f1: 79.562 val-f1: 85.000\n",
      "epoch: 019 | train loss: 0.536 | val loss: 0.461 | train-f1: 79.000 val-f1: 86.500\n",
      "epoch: 020 | train loss: 0.511 | val loss: 0.466 | train-f1: 84.312 val-f1: 85.000\n",
      "epoch: 021 | train loss: 0.523 | val loss: 0.456 | train-f1: 80.938 val-f1: 85.000\n",
      "epoch: 022 | train loss: 0.504 | val loss: 0.456 | train-f1: 83.125 val-f1: 85.000\n",
      "epoch: 023 | train loss: 0.520 | val loss: 0.459 | train-f1: 81.125 val-f1: 85.000\n",
      "epoch: 024 | train loss: 0.505 | val loss: 0.446 | train-f1: 80.562 val-f1: 85.000\n",
      "epoch: 025 | train loss: 0.492 | val loss: 0.448 | train-f1: 86.562 val-f1: 85.000\n",
      "epoch: 026 | train loss: 0.480 | val loss: 0.446 | train-f1: 84.125 val-f1: 84.000\n",
      "epoch: 027 | train loss: 0.489 | val loss: 0.456 | train-f1: 82.375 val-f1: 85.000\n",
      "epoch: 028 | train loss: 0.494 | val loss: 0.445 | train-f1: 79.438 val-f1: 85.000\n",
      "epoch: 029 | train loss: 0.458 | val loss: 0.429 | train-f1: 85.188 val-f1: 84.000\n",
      "epoch: 030 | train loss: 0.475 | val loss: 0.436 | train-f1: 82.625 val-f1: 85.000\n",
      "epoch: 031 | train loss: 0.482 | val loss: 0.422 | train-f1: 82.250 val-f1: 86.500\n",
      "epoch: 032 | train loss: 0.486 | val loss: 0.438 | train-f1: 82.375 val-f1: 85.000\n",
      "epoch: 033 | train loss: 0.457 | val loss: 0.439 | train-f1: 84.438 val-f1: 84.000\n",
      "epoch: 034 | train loss: 0.459 | val loss: 0.420 | train-f1: 84.062 val-f1: 87.250\n",
      "epoch: 035 | train loss: 0.468 | val loss: 0.422 | train-f1: 82.875 val-f1: 86.500\n",
      "epoch: 036 | train loss: 0.445 | val loss: 0.427 | train-f1: 84.188 val-f1: 85.000\n",
      "epoch: 037 | train loss: 0.438 | val loss: 0.427 | train-f1: 86.312 val-f1: 84.000\n",
      "epoch: 038 | train loss: 0.456 | val loss: 0.424 | train-f1: 85.000 val-f1: 85.750\n",
      "epoch: 039 | train loss: 0.434 | val loss: 0.415 | train-f1: 87.688 val-f1: 88.250\n",
      "epoch: 040 | train loss: 0.440 | val loss: 0.413 | train-f1: 86.438 val-f1: 87.250\n",
      "epoch: 041 | train loss: 0.448 | val loss: 0.407 | train-f1: 84.438 val-f1: 86.250\n",
      "epoch: 042 | train loss: 0.439 | val loss: 0.406 | train-f1: 86.562 val-f1: 85.750\n",
      "epoch: 043 | train loss: 0.427 | val loss: 0.399 | train-f1: 88.188 val-f1: 87.250\n",
      "epoch: 044 | train loss: 0.446 | val loss: 0.410 | train-f1: 84.875 val-f1: 87.250\n",
      "epoch: 045 | train loss: 0.406 | val loss: 0.407 | train-f1: 86.062 val-f1: 85.750\n",
      "epoch: 046 | train loss: 0.429 | val loss: 0.417 | train-f1: 85.812 val-f1: 85.750\n",
      "epoch: 047 | train loss: 0.394 | val loss: 0.401 | train-f1: 89.312 val-f1: 87.250\n",
      "epoch: 048 | train loss: 0.397 | val loss: 0.404 | train-f1: 88.875 val-f1: 87.250\n",
      "epoch: 049 | train loss: 0.387 | val loss: 0.421 | train-f1: 89.500 val-f1: 85.750\n",
      "epoch: 050 | train loss: 0.431 | val loss: 0.401 | train-f1: 84.438 val-f1: 85.750\n",
      "epoch: 051 | train loss: 0.397 | val loss: 0.394 | train-f1: 89.688 val-f1: 84.750\n",
      "epoch: 052 | train loss: 0.374 | val loss: 0.393 | train-f1: 90.812 val-f1: 86.250\n",
      "epoch: 053 | train loss: 0.380 | val loss: 0.393 | train-f1: 90.188 val-f1: 86.250\n",
      "epoch: 054 | train loss: 0.384 | val loss: 0.391 | train-f1: 89.938 val-f1: 86.250\n",
      "epoch: 055 | train loss: 0.369 | val loss: 0.397 | train-f1: 90.500 val-f1: 86.250\n",
      "epoch: 056 | train loss: 0.390 | val loss: 0.383 | train-f1: 88.938 val-f1: 86.250\n",
      "epoch: 057 | train loss: 0.360 | val loss: 0.391 | train-f1: 89.000 val-f1: 84.750\n",
      "epoch: 058 | train loss: 0.364 | val loss: 0.400 | train-f1: 89.250 val-f1: 84.750\n",
      "epoch: 059 | train loss: 0.378 | val loss: 0.395 | train-f1: 87.750 val-f1: 86.250\n",
      "epoch: 060 | train loss: 0.347 | val loss: 0.381 | train-f1: 90.312 val-f1: 86.250\n",
      "epoch: 061 | train loss: 0.346 | val loss: 0.371 | train-f1: 90.000 val-f1: 86.250\n",
      "epoch: 062 | train loss: 0.341 | val loss: 0.383 | train-f1: 90.062 val-f1: 86.250\n",
      "epoch: 063 | train loss: 0.346 | val loss: 0.377 | train-f1: 90.125 val-f1: 86.250\n",
      "epoch: 064 | train loss: 0.349 | val loss: 0.386 | train-f1: 89.625 val-f1: 86.250\n",
      "epoch: 065 | train loss: 0.346 | val loss: 0.383 | train-f1: 90.125 val-f1: 87.500\n",
      "epoch: 066 | train loss: 0.329 | val loss: 0.369 | train-f1: 91.188 val-f1: 86.250\n",
      "epoch: 067 | train loss: 0.330 | val loss: 0.380 | train-f1: 90.250 val-f1: 86.250\n",
      "epoch: 068 | train loss: 0.328 | val loss: 0.367 | train-f1: 91.688 val-f1: 86.250\n",
      "epoch: 069 | train loss: 0.331 | val loss: 0.366 | train-f1: 90.688 val-f1: 86.250\n",
      "epoch: 070 | train loss: 0.320 | val loss: 0.367 | train-f1: 91.125 val-f1: 86.250\n",
      "epoch: 071 | train loss: 0.336 | val loss: 0.362 | train-f1: 90.562 val-f1: 86.250\n",
      "epoch: 072 | train loss: 0.332 | val loss: 0.351 | train-f1: 91.812 val-f1: 86.250\n",
      "epoch: 073 | train loss: 0.345 | val loss: 0.352 | train-f1: 87.188 val-f1: 86.250\n",
      "epoch: 074 | train loss: 0.338 | val loss: 0.353 | train-f1: 90.125 val-f1: 86.250\n",
      "epoch: 075 | train loss: 0.310 | val loss: 0.349 | train-f1: 91.375 val-f1: 86.250\n",
      "epoch: 076 | train loss: 0.291 | val loss: 0.359 | train-f1: 90.750 val-f1: 87.500\n",
      "epoch: 077 | train loss: 0.340 | val loss: 0.355 | train-f1: 90.625 val-f1: 86.250\n",
      "epoch: 078 | train loss: 0.349 | val loss: 0.342 | train-f1: 89.250 val-f1: 86.250\n",
      "epoch: 079 | train loss: 0.310 | val loss: 0.358 | train-f1: 90.062 val-f1: 87.500\n",
      "epoch: 080 | train loss: 0.310 | val loss: 0.359 | train-f1: 92.000 val-f1: 87.500\n",
      "epoch: 081 | train loss: 0.321 | val loss: 0.352 | train-f1: 89.812 val-f1: 87.500\n",
      "epoch: 082 | train loss: 0.321 | val loss: 0.349 | train-f1: 90.188 val-f1: 87.500\n",
      "epoch: 083 | train loss: 0.289 | val loss: 0.343 | train-f1: 92.750 val-f1: 87.500\n",
      "epoch: 084 | train loss: 0.306 | val loss: 0.349 | train-f1: 91.812 val-f1: 87.500\n",
      "epoch: 085 | train loss: 0.275 | val loss: 0.344 | train-f1: 93.875 val-f1: 87.500\n",
      "epoch: 086 | train loss: 0.291 | val loss: 0.334 | train-f1: 91.188 val-f1: 87.500\n",
      "epoch: 087 | train loss: 0.284 | val loss: 0.352 | train-f1: 92.562 val-f1: 87.500\n",
      "epoch: 088 | train loss: 0.310 | val loss: 0.343 | train-f1: 90.625 val-f1: 87.500\n",
      "epoch: 089 | train loss: 0.280 | val loss: 0.339 | train-f1: 91.812 val-f1: 87.500\n",
      "epoch: 090 | train loss: 0.255 | val loss: 0.326 | train-f1: 94.250 val-f1: 87.500\n",
      "epoch: 091 | train loss: 0.282 | val loss: 0.330 | train-f1: 94.500 val-f1: 87.500\n",
      "epoch: 092 | train loss: 0.281 | val loss: 0.325 | train-f1: 92.625 val-f1: 87.500\n",
      "epoch: 093 | train loss: 0.249 | val loss: 0.330 | train-f1: 95.188 val-f1: 88.750\n",
      "epoch: 094 | train loss: 0.279 | val loss: 0.328 | train-f1: 93.000 val-f1: 88.750\n",
      "epoch: 095 | train loss: 0.264 | val loss: 0.323 | train-f1: 92.938 val-f1: 88.750\n",
      "epoch: 096 | train loss: 0.256 | val loss: 0.320 | train-f1: 93.812 val-f1: 88.750\n",
      "epoch: 097 | train loss: 0.280 | val loss: 0.321 | train-f1: 92.688 val-f1: 88.750\n",
      "epoch: 098 | train loss: 0.274 | val loss: 0.318 | train-f1: 94.062 val-f1: 88.750\n",
      "epoch: 099 | train loss: 0.269 | val loss: 0.322 | train-f1: 93.438 val-f1: 88.750\n",
      "epoch: 100 | train loss: 0.270 | val loss: 0.314 | train-f1: 91.125 val-f1: 88.750\n",
      "epoch: 101 | train loss: 0.273 | val loss: 0.329 | train-f1: 91.250 val-f1: 88.750\n",
      "epoch: 102 | train loss: 0.230 | val loss: 0.328 | train-f1: 94.562 val-f1: 88.750\n",
      "epoch: 103 | train loss: 0.256 | val loss: 0.321 | train-f1: 92.188 val-f1: 88.750\n",
      "epoch: 104 | train loss: 0.276 | val loss: 0.317 | train-f1: 91.750 val-f1: 88.750\n",
      "epoch: 105 | train loss: 0.265 | val loss: 0.319 | train-f1: 91.188 val-f1: 88.750\n",
      "epoch: 106 | train loss: 0.263 | val loss: 0.315 | train-f1: 93.438 val-f1: 88.750\n",
      "epoch: 107 | train loss: 0.250 | val loss: 0.311 | train-f1: 92.750 val-f1: 88.750\n",
      "epoch: 108 | train loss: 0.238 | val loss: 0.305 | train-f1: 92.812 val-f1: 88.750\n",
      "epoch: 109 | train loss: 0.253 | val loss: 0.312 | train-f1: 93.938 val-f1: 88.750\n",
      "epoch: 110 | train loss: 0.263 | val loss: 0.312 | train-f1: 92.562 val-f1: 88.750\n",
      "epoch: 111 | train loss: 0.246 | val loss: 0.308 | train-f1: 93.625 val-f1: 88.750\n",
      "epoch: 112 | train loss: 0.264 | val loss: 0.305 | train-f1: 92.250 val-f1: 88.750\n",
      "epoch: 113 | train loss: 0.240 | val loss: 0.302 | train-f1: 94.438 val-f1: 88.750\n",
      "epoch: 114 | train loss: 0.233 | val loss: 0.302 | train-f1: 93.062 val-f1: 88.750\n",
      "epoch: 115 | train loss: 0.239 | val loss: 0.306 | train-f1: 93.062 val-f1: 88.750\n",
      "epoch: 116 | train loss: 0.205 | val loss: 0.307 | train-f1: 94.500 val-f1: 88.750\n",
      "epoch: 117 | train loss: 0.220 | val loss: 0.295 | train-f1: 94.875 val-f1: 88.750\n",
      "epoch: 118 | train loss: 0.208 | val loss: 0.292 | train-f1: 94.250 val-f1: 90.250\n",
      "epoch: 119 | train loss: 0.236 | val loss: 0.299 | train-f1: 93.688 val-f1: 88.750\n",
      "epoch: 120 | train loss: 0.208 | val loss: 0.299 | train-f1: 93.938 val-f1: 88.750\n",
      "epoch: 121 | train loss: 0.258 | val loss: 0.298 | train-f1: 92.938 val-f1: 88.750\n",
      "epoch: 122 | train loss: 0.268 | val loss: 0.301 | train-f1: 92.125 val-f1: 88.750\n",
      "epoch: 123 | train loss: 0.262 | val loss: 0.308 | train-f1: 93.062 val-f1: 88.750\n",
      "epoch: 124 | train loss: 0.212 | val loss: 0.291 | train-f1: 95.375 val-f1: 93.000\n",
      "epoch: 125 | train loss: 0.227 | val loss: 0.300 | train-f1: 95.062 val-f1: 88.750\n",
      "epoch: 126 | train loss: 0.202 | val loss: 0.303 | train-f1: 96.562 val-f1: 88.750\n",
      "epoch: 127 | train loss: 0.208 | val loss: 0.296 | train-f1: 95.125 val-f1: 91.750\n",
      "epoch: 128 | train loss: 0.198 | val loss: 0.294 | train-f1: 95.125 val-f1: 91.750\n",
      "epoch: 129 | train loss: 0.208 | val loss: 0.283 | train-f1: 95.812 val-f1: 88.750\n",
      "epoch: 130 | train loss: 0.202 | val loss: 0.296 | train-f1: 94.438 val-f1: 88.750\n",
      "epoch: 131 | train loss: 0.208 | val loss: 0.282 | train-f1: 94.312 val-f1: 91.750\n",
      "epoch: 132 | train loss: 0.205 | val loss: 0.289 | train-f1: 95.125 val-f1: 88.750\n",
      "epoch: 133 | train loss: 0.192 | val loss: 0.288 | train-f1: 95.250 val-f1: 88.750\n",
      "epoch: 134 | train loss: 0.198 | val loss: 0.286 | train-f1: 94.750 val-f1: 93.000\n",
      "epoch: 135 | train loss: 0.227 | val loss: 0.273 | train-f1: 92.938 val-f1: 92.750\n",
      "epoch: 136 | train loss: 0.185 | val loss: 0.286 | train-f1: 95.812 val-f1: 91.500\n",
      "epoch: 137 | train loss: 0.190 | val loss: 0.281 | train-f1: 95.625 val-f1: 91.750\n",
      "epoch: 138 | train loss: 0.206 | val loss: 0.278 | train-f1: 93.688 val-f1: 93.000\n",
      "epoch: 139 | train loss: 0.193 | val loss: 0.295 | train-f1: 95.188 val-f1: 88.750\n",
      "epoch: 140 | train loss: 0.193 | val loss: 0.281 | train-f1: 95.500 val-f1: 91.750\n",
      "epoch: 141 | train loss: 0.202 | val loss: 0.276 | train-f1: 94.750 val-f1: 91.750\n",
      "epoch: 142 | train loss: 0.178 | val loss: 0.283 | train-f1: 96.688 val-f1: 88.750\n",
      "epoch: 143 | train loss: 0.187 | val loss: 0.276 | train-f1: 95.375 val-f1: 91.750\n",
      "epoch: 144 | train loss: 0.217 | val loss: 0.264 | train-f1: 93.375 val-f1: 91.750\n",
      "epoch: 145 | train loss: 0.218 | val loss: 0.277 | train-f1: 95.188 val-f1: 93.000\n",
      "epoch: 146 | train loss: 0.178 | val loss: 0.281 | train-f1: 95.688 val-f1: 88.750\n",
      "epoch: 147 | train loss: 0.170 | val loss: 0.279 | train-f1: 95.875 val-f1: 91.500\n",
      "epoch: 148 | train loss: 0.201 | val loss: 0.270 | train-f1: 94.312 val-f1: 91.750\n",
      "epoch: 149 | train loss: 0.172 | val loss: 0.280 | train-f1: 95.875 val-f1: 88.750\n",
      "epoch: 150 | train loss: 0.200 | val loss: 0.275 | train-f1: 93.375 val-f1: 91.750\n",
      "epoch: 151 | train loss: 0.194 | val loss: 0.291 | train-f1: 94.938 val-f1: 88.750\n",
      "epoch: 152 | train loss: 0.183 | val loss: 0.282 | train-f1: 95.875 val-f1: 90.250\n",
      "epoch: 153 | train loss: 0.170 | val loss: 0.285 | train-f1: 96.938 val-f1: 90.250\n",
      "epoch: 154 | train loss: 0.192 | val loss: 0.286 | train-f1: 94.312 val-f1: 91.750\n",
      "epoch: 155 | train loss: 0.192 | val loss: 0.277 | train-f1: 94.375 val-f1: 94.250\n",
      "epoch: 156 | train loss: 0.236 | val loss: 0.267 | train-f1: 90.875 val-f1: 94.250\n",
      "epoch: 157 | train loss: 0.176 | val loss: 0.266 | train-f1: 96.062 val-f1: 94.250\n",
      "epoch: 158 | train loss: 0.166 | val loss: 0.262 | train-f1: 96.000 val-f1: 94.250\n",
      "epoch: 159 | train loss: 0.152 | val loss: 0.263 | train-f1: 96.625 val-f1: 94.250\n",
      "epoch: 160 | train loss: 0.174 | val loss: 0.276 | train-f1: 97.250 val-f1: 91.750\n",
      "epoch: 161 | train loss: 0.160 | val loss: 0.273 | train-f1: 97.500 val-f1: 93.000\n",
      "epoch: 162 | train loss: 0.150 | val loss: 0.274 | train-f1: 95.750 val-f1: 94.250\n",
      "epoch: 163 | train loss: 0.174 | val loss: 0.268 | train-f1: 95.375 val-f1: 93.000\n",
      "epoch: 164 | train loss: 0.179 | val loss: 0.261 | train-f1: 95.250 val-f1: 93.000\n",
      "epoch: 165 | train loss: 0.156 | val loss: 0.265 | train-f1: 95.125 val-f1: 94.250\n",
      "epoch: 166 | train loss: 0.143 | val loss: 0.265 | train-f1: 96.438 val-f1: 91.750\n",
      "epoch: 167 | train loss: 0.154 | val loss: 0.272 | train-f1: 96.625 val-f1: 91.750\n",
      "epoch: 168 | train loss: 0.145 | val loss: 0.254 | train-f1: 97.188 val-f1: 94.250\n",
      "epoch: 169 | train loss: 0.155 | val loss: 0.256 | train-f1: 96.250 val-f1: 93.000\n",
      "epoch: 170 | train loss: 0.159 | val loss: 0.253 | train-f1: 96.312 val-f1: 93.000\n",
      "epoch: 171 | train loss: 0.159 | val loss: 0.254 | train-f1: 94.938 val-f1: 94.250\n",
      "epoch: 172 | train loss: 0.144 | val loss: 0.266 | train-f1: 97.312 val-f1: 91.750\n",
      "epoch: 173 | train loss: 0.141 | val loss: 0.259 | train-f1: 97.625 val-f1: 93.000\n",
      "epoch: 174 | train loss: 0.153 | val loss: 0.265 | train-f1: 95.312 val-f1: 91.750\n",
      "epoch: 175 | train loss: 0.145 | val loss: 0.269 | train-f1: 98.188 val-f1: 90.250\n",
      "epoch: 176 | train loss: 0.170 | val loss: 0.255 | train-f1: 96.000 val-f1: 91.750\n",
      "epoch: 177 | train loss: 0.133 | val loss: 0.254 | train-f1: 97.500 val-f1: 94.250\n",
      "epoch: 178 | train loss: 0.150 | val loss: 0.259 | train-f1: 97.250 val-f1: 91.750\n",
      "epoch: 179 | train loss: 0.140 | val loss: 0.271 | train-f1: 97.562 val-f1: 91.750\n",
      "epoch: 180 | train loss: 0.143 | val loss: 0.271 | train-f1: 96.938 val-f1: 91.750\n",
      "epoch: 181 | train loss: 0.177 | val loss: 0.266 | train-f1: 96.688 val-f1: 91.750\n",
      "epoch: 182 | train loss: 0.175 | val loss: 0.260 | train-f1: 96.562 val-f1: 94.250\n",
      "epoch: 183 | train loss: 0.130 | val loss: 0.260 | train-f1: 97.375 val-f1: 91.750\n",
      "epoch: 184 | train loss: 0.142 | val loss: 0.266 | train-f1: 96.875 val-f1: 91.750\n",
      "epoch: 185 | train loss: 0.130 | val loss: 0.261 | train-f1: 98.062 val-f1: 93.000\n",
      "epoch: 186 | train loss: 0.137 | val loss: 0.256 | train-f1: 97.625 val-f1: 94.250\n",
      "epoch: 187 | train loss: 0.131 | val loss: 0.255 | train-f1: 97.062 val-f1: 94.250\n",
      "epoch: 188 | train loss: 0.125 | val loss: 0.260 | train-f1: 98.500 val-f1: 94.250\n",
      "epoch: 189 | train loss: 0.164 | val loss: 0.269 | train-f1: 94.938 val-f1: 94.250\n",
      "epoch: 190 | train loss: 0.161 | val loss: 0.265 | train-f1: 96.500 val-f1: 91.500\n",
      "Saving best model weights!\n",
      "epoch: 191 | train loss: 0.146 | val loss: 0.263 | train-f1: 96.438 val-f1: 93.000\n",
      "epoch: 192 | train loss: 0.143 | val loss: 0.262 | train-f1: 96.750 val-f1: 91.500\n",
      "epoch: 193 | train loss: 0.130 | val loss: 0.270 | train-f1: 98.250 val-f1: 91.750\n",
      "epoch: 194 | train loss: 0.177 | val loss: 0.268 | train-f1: 95.562 val-f1: 91.750\n",
      "epoch: 195 | train loss: 0.123 | val loss: 0.258 | train-f1: 99.438 val-f1: 93.000\n",
      "epoch: 196 | train loss: 0.157 | val loss: 0.266 | train-f1: 97.000 val-f1: 91.750\n",
      "epoch: 197 | train loss: 0.138 | val loss: 0.262 | train-f1: 96.562 val-f1: 93.000\n",
      "epoch: 198 | train loss: 0.148 | val loss: 0.262 | train-f1: 97.812 val-f1: 91.750\n",
      "epoch: 199 | train loss: 0.133 | val loss: 0.264 | train-f1: 97.625 val-f1: 91.750\n",
      "epoch: 200 | train loss: 0.204 | val loss: 0.266 | train-f1: 94.438 val-f1: 91.750\n",
      "Total Time: 17.076\n"
     ]
    }
   ],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(12, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = BinaryClassification()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create Parameter Dictionary\n",
    "params_train={\n",
    "  \"train\": train_loader,\n",
    "    \"val\": val_loader,\n",
    " \"epochs\": 200,\n",
    " \"optimiser\": opt,\n",
    " \"lr_change\": ReduceLROnPlateau(opt,\n",
    "                                mode='min',\n",
    "                                factor=0.5,\n",
    "                                patience=20,verbose=0),\n",
    " \"f_loss\": loss,\n",
    " \"weight_path\": \"weights.pt\",\n",
    " \"eval_func\" : ['accuracy','f1','recall','precision'],\n",
    " \"write_metric\" : 'f1'\n",
    "}\n",
    "\n",
    "nn_model,loss_hist,metric_hist=train_val(model,params_train,verbose=True)\n",
    "epochs=params_train[\"epochs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76030fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:29:27.183266Z",
     "iopub.status.busy": "2022-07-02T11:29:27.182325Z",
     "iopub.status.idle": "2022-07-02T11:29:27.301716Z",
     "shell.execute_reply": "2022-07-02T11:29:27.300662Z"
    },
    "papermill": {
     "duration": 0.134515,
     "end_time": "2022-07-02T11:29:27.304257",
     "exception": false,
     "start_time": "2022-07-02T11:29:27.169742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"a11b83cb-3d81-4cf3-8a6f-7f8d3f0897d1\" class=\"plotly-graph-div\" style=\"height:525px; width:1200px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a11b83cb-3d81-4cf3-8a6f-7f8d3f0897d1\")) {                    Plotly.newPlot(                        \"a11b83cb-3d81-4cf3-8a6f-7f8d3f0897d1\",                        [{\"line\":{\"color\":\"#94D4F6\",\"width\":2},\"name\":\"train-loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],\"y\":[0.8005795329809189,0.7627971842885017,0.7203499116003513,0.6826777048408985,0.6725172027945518,0.6539669223129749,0.622519712895155,0.6149066016077995,0.6145225185900927,0.6024361923336983,0.5927163399755955,0.5652294047176838,0.5822862926870584,0.5426008757203817,0.5616909675300121,0.5327486712485552,0.5333493147045374,0.5330127775669098,0.5359015837311745,0.5106793958693743,0.5231223944574594,0.5035319831222296,0.5203308742493391,0.5046128481626511,0.49191151931881905,0.4797808825969696,0.4886538293212652,0.4940283074975014,0.45846715942025185,0.4749365523457527,0.48178087174892426,0.4861633740365505,0.4574444405734539,0.45854105055332184,0.468373604118824,0.4454554785043001,0.43842111714184284,0.4563402198255062,0.4336129855364561,0.44008689001202583,0.44823645800352097,0.4392981082201004,0.426626730710268,0.44600851461291313,0.40630132146179676,0.4291521143168211,0.39426024444401264,0.3973742229864001,0.3868278209120035,0.43113151378929615,0.39746550284326077,0.3741980576887727,0.3797195293009281,0.3835828881710768,0.3694288022816181,0.3900367086753249,0.36043339036405087,0.36387405544519424,0.378461143001914,0.3471767157316208,0.345629433169961,0.34065086115151644,0.3464766666293144,0.3491648342460394,0.3458077209070325,0.3289776770398021,0.3298360472545028,0.32835272140800953,0.33073904924094677,0.3198533467948437,0.33552595414221287,0.33184574730694294,0.3454647734761238,0.3383097145706415,0.31009222008287907,0.29092384688556194,0.3404795387759805,0.34939590003341436,0.31036865524947643,0.3099847240373492,0.32146310340613127,0.3214420070871711,0.28929070848971605,0.30623725708574057,0.27463705372065306,0.29052835423499346,0.2835990507155657,0.31028693029657006,0.28015036415308714,0.2554203048348427,0.282069131731987,0.2812367985025048,0.2487574266269803,0.2790297819301486,0.2643936211243272,0.2563914395868778,0.2802576059475541,0.2742031617090106,0.2692058589309454,0.27041084319353104,0.27272485196590424,0.23044817289337516,0.25572175811976194,0.2761228093877435,0.2645697761327028,0.2626599920913577,0.2504837419837713,0.23774662613868713,0.25321357138454914,0.2630843482911587,0.24624567665159702,0.26376942824572325,0.2401648685336113,0.23307730304077268,0.23878590296953917,0.20472493628039956,0.2195266904309392,0.20810327026993036,0.2360947895795107,0.20807175524532795,0.2578290347009897,0.2684999220073223,0.2624242268502712,0.2120407810434699,0.22732765972614288,0.20162558369338512,0.20755481999367476,0.19778522150591016,0.20788179757073522,0.20176541479304433,0.20791669422760606,0.2053467994555831,0.19226216105744243,0.19817340280860662,0.2270087874494493,0.1849599783308804,0.19044953677803278,0.2063312097452581,0.19262118637561798,0.19275961630046368,0.2015653341077268,0.1784293525852263,0.1872410555370152,0.21657866751775146,0.2175165405496955,0.17784266266971827,0.16985591920092702,0.2005110695026815,0.1716059218160808,0.20043057948350906,0.1943655596114695,0.1830387837253511,0.17016032012179494,0.1920541450381279,0.1921874643303454,0.2358292550779879,0.17569027980789542,0.16562943952158093,0.1520109623670578,0.17368364380672574,0.15966667467728257,0.15045036701485515,0.17401465494185686,0.17916359147056937,0.1560130207799375,0.14319912903010845,0.1541125348303467,0.14459604304283857,0.15535355336032808,0.1593468924984336,0.15926261665299535,0.1439280048944056,0.14103890489786863,0.15307321306318045,0.1452221591025591,0.17048182897269726,0.1325684932526201,0.15023963595740497,0.1400483374018222,0.14291451405733824,0.17709605628624558,0.175213442184031,0.13016174687072635,0.14168248232454062,0.130214327480644,0.13682371866889298,0.1312554981559515,0.12464479077607393,0.1640647512394935,0.1608863128349185,0.14600635366514325,0.1429929849691689,0.130272779148072,0.1767009301111102,0.12255219090729952,0.15682481252588332,0.138225216884166,0.14819906651973724,0.13323668763041496,0.203526115976274],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"marker\":{\"line\":{\"color\":\"black\",\"width\":3},\"cmid\":0,\"reversescale\":true,\"showscale\":true,\"size\":10},\"opacity\":1.0},{\"line\":{\"color\":\"#94D4F6\",\"width\":2},\"name\":\"train-f1\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],\"y\":[47.4375,50.0625,60.25,60.0,61.8125,61.4375,72.875,71.6875,74.25,73.4375,75.75,76.75,76.5,79.6875,78.5,78.25,81.0625,79.5625,79.0,84.3125,80.9375,83.125,81.125,80.5625,86.5625,84.125,82.375,79.4375,85.1875,82.625,82.25,82.375,84.4375,84.0625,82.875,84.1875,86.3125,85.0,87.6875,86.4375,84.4375,86.5625,88.1875,84.875,86.0625,85.8125,89.3125,88.875,89.5,84.4375,89.6875,90.8125,90.1875,89.9375,90.5,88.9375,89.0,89.25,87.75,90.3125,90.0,90.0625,90.125,89.625,90.125,91.1875,90.25,91.6875,90.6875,91.125,90.5625,91.8125,87.1875,90.125,91.375,90.75,90.625,89.25,90.0625,92.0,89.8125,90.1875,92.75,91.8125,93.875,91.1875,92.5625,90.625,91.8125,94.25,94.5,92.625,95.1875,93.0,92.9375,93.8125,92.6875,94.0625,93.4375,91.125,91.25,94.5625,92.1875,91.75,91.1875,93.4375,92.75,92.8125,93.9375,92.5625,93.625,92.25,94.4375,93.0625,93.0625,94.5,94.875,94.25,93.6875,93.9375,92.9375,92.125,93.0625,95.375,95.0625,96.5625,95.125,95.125,95.8125,94.4375,94.3125,95.125,95.25,94.75,92.9375,95.8125,95.625,93.6875,95.1875,95.5,94.75,96.6875,95.375,93.375,95.1875,95.6875,95.875,94.3125,95.875,93.375,94.9375,95.875,96.9375,94.3125,94.375,90.875,96.0625,96.0,96.625,97.25,97.5,95.75,95.375,95.25,95.125,96.4375,96.625,97.1875,96.25,96.3125,94.9375,97.3125,97.625,95.3125,98.1875,96.0,97.5,97.25,97.5625,96.9375,96.6875,96.5625,97.375,96.875,98.0625,97.625,97.0625,98.5,94.9375,96.5,96.4375,96.75,98.25,95.5625,99.4375,97.0,96.5625,97.8125,97.625,94.4375],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\",\"marker\":{\"line\":{\"color\":\"black\",\"width\":3},\"cmid\":0,\"reversescale\":true,\"showscale\":true,\"size\":10},\"opacity\":1.0},{\"line\":{\"color\":\"#454545\",\"width\":2},\"name\":\"val-loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],\"y\":[0.6965803503990173,0.652972549200058,0.6216521859169006,0.5947294235229492,0.5736770331859589,0.5543251633644104,0.5406464263796806,0.531256653368473,0.5206809788942337,0.515618234872818,0.5039964988827705,0.4996369183063507,0.49422069638967514,0.4852803498506546,0.4845988303422928,0.4787316545844078,0.4808191508054733,0.47058896720409393,0.46062561124563217,0.4657256230711937,0.45569587498903275,0.4557621404528618,0.45925821363925934,0.446220226585865,0.4478478953242302,0.44618793576955795,0.4563894048333168,0.4452352225780487,0.42925237119197845,0.43556593358516693,0.42156337946653366,0.438282735645771,0.4385777935385704,0.4195220172405243,0.42157042026519775,0.42747972160577774,0.4267178103327751,0.4237591251730919,0.41523929685354233,0.4126077741384506,0.4072783961892128,0.40634847432374954,0.39930540323257446,0.4097345322370529,0.4065656289458275,0.41695239394903183,0.4014151394367218,0.40398990362882614,0.4212499186396599,0.4005096033215523,0.3943849503993988,0.39254502952098846,0.3928648680448532,0.3907213360071182,0.3965815082192421,0.38278181105852127,0.3905670866370201,0.3999946787953377,0.3951040878891945,0.3809461295604706,0.3706355467438698,0.3830897882580757,0.37727752327919006,0.3862372636795044,0.38265761733055115,0.3691859543323517,0.38007090240716934,0.36725128442049026,0.3661837503314018,0.366721011698246,0.3619268164038658,0.3505851849913597,0.35181206464767456,0.3532876670360565,0.34925490617752075,0.35859130322933197,0.35476911813020706,0.3419521823525429,0.35750630497932434,0.35894038528203964,0.3522823676466942,0.34909704327583313,0.34345032274723053,0.3493848592042923,0.34364770352840424,0.33391284197568893,0.35232583433389664,0.3429503068327904,0.33930835127830505,0.32561006397008896,0.3297741040587425,0.32450734078884125,0.32994716614484787,0.3279793858528137,0.3234354853630066,0.3203275352716446,0.3214784115552902,0.3175734728574753,0.32175054401159286,0.3139384388923645,0.32911261171102524,0.3284071832895279,0.3209231272339821,0.31690719723701477,0.31949251145124435,0.3150044083595276,0.31127209961414337,0.30490104109048843,0.31212297081947327,0.31247345730662346,0.30838850513100624,0.3053497336804867,0.3017626293003559,0.3017890565097332,0.30642900988459587,0.3071896471083164,0.29527081921696663,0.2922869212925434,0.2986658625304699,0.2987888231873512,0.29762812331318855,0.30145280808210373,0.30836841464042664,0.29106152430176735,0.3004530444741249,0.30270159989595413,0.2956485189497471,0.29414331167936325,0.2827947102487087,0.29565369337797165,0.2823677100241184,0.2892192453145981,0.28812750801444054,0.2861866280436516,0.2729492224752903,0.28558122739195824,0.28130558133125305,0.27844080328941345,0.2949415035545826,0.28076939657330513,0.2757057957351208,0.28312306106090546,0.27598026394844055,0.264496099203825,0.27724600955843925,0.28146959841251373,0.27868056297302246,0.2696024514734745,0.2796238288283348,0.2748648338019848,0.2907579392194748,0.2815145291388035,0.28529762104153633,0.2864547483623028,0.2773115485906601,0.26672663912177086,0.26603541523218155,0.2619618922472,0.262632355093956,0.27647218480706215,0.27251341566443443,0.273881983011961,0.2683393061161041,0.2609596699476242,0.26498448476195335,0.2647674046456814,0.2717606723308563,0.25443411245942116,0.2557787112891674,0.2527502290904522,0.2539798282086849,0.2660585083067417,0.2591843381524086,0.2651706635951996,0.26854927092790604,0.25489045679569244,0.2539222091436386,0.25858164206147194,0.27144627645611763,0.2713201865553856,0.26572001725435257,0.26033222302794456,0.26043881103396416,0.26596932858228683,0.261271670460701,0.2562940642237663,0.2554820440709591,0.2604479268193245,0.268596526235342,0.2651694416999817,0.2629922144114971,0.26176416873931885,0.2703133709728718,0.2681066356599331,0.2578030116856098,0.26592111214995384,0.26221900433301926,0.2617248073220253,0.26361487060785294,0.26558917760849],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"marker\":{\"line\":{\"color\":\"black\",\"width\":3},\"cmid\":0,\"reversescale\":true,\"showscale\":true,\"size\":10},\"opacity\":1.0},{\"line\":{\"color\":\"#454545\",\"width\":2},\"name\":\"val-f1\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],\"y\":[66.5,71.5,70.0,72.25,76.75,82.75,81.5,81.5,85.5,86.75,85.0,85.0,84.0,84.0,83.25,82.25,85.0,85.0,86.5,85.0,85.0,85.0,85.0,85.0,85.0,84.0,85.0,85.0,84.0,85.0,86.5,85.0,84.0,87.25,86.5,85.0,84.0,85.75,88.25,87.25,86.25,85.75,87.25,87.25,85.75,85.75,87.25,87.25,85.75,85.75,84.75,86.25,86.25,86.25,86.25,86.25,84.75,84.75,86.25,86.25,86.25,86.25,86.25,86.25,87.5,86.25,86.25,86.25,86.25,86.25,86.25,86.25,86.25,86.25,86.25,87.5,86.25,86.25,87.5,87.5,87.5,87.5,87.5,87.5,87.5,87.5,87.5,87.5,87.5,87.5,87.5,87.5,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,88.75,90.25,88.75,88.75,88.75,88.75,88.75,93.0,88.75,88.75,91.75,91.75,88.75,88.75,91.75,88.75,88.75,93.0,92.75,91.5,91.75,93.0,88.75,91.75,91.75,88.75,91.75,91.75,93.0,88.75,91.5,91.75,88.75,91.75,88.75,90.25,90.25,91.75,94.25,94.25,94.25,94.25,94.25,91.75,93.0,94.25,93.0,93.0,94.25,91.75,91.75,94.25,93.0,93.0,94.25,91.75,93.0,91.75,90.25,91.75,94.25,91.75,91.75,91.75,91.75,94.25,91.75,91.75,93.0,94.25,94.25,94.25,94.25,91.5,93.0,91.5,91.75,91.75,93.0,91.75,93.0,91.75,91.75,91.75],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\",\"marker\":{\"line\":{\"color\":\"black\",\"width\":3},\"cmid\":0,\"reversescale\":true,\"showscale\":true,\"size\":10},\"opacity\":1.0}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"lost_hist\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"metric_f1\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"font\":{\"family\":\"sans-serif\",\"size\":12},\"title\":{\"text\":\"Train / Validation Data Splitting\"},\"width\":1200,\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(176, 242, 188)\"],[0.16666666666666666,\"rgb(137, 232, 172)\"],[0.3333333333333333,\"rgb(103, 219, 165)\"],[0.5,\"rgb(76, 200, 163)\"],[0.6666666666666666,\"rgb(56, 178, 163)\"],[0.8333333333333334,\"rgb(44, 152, 160)\"],[1.0,\"rgb(37, 125, 152)\"]],\"showscale\":false}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a11b83cb-3d81-4cf3-8a6f-7f8d3f0897d1');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_res(metric_hist,name):\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2,\n",
    "                        subplot_titles=['lost_hist',f'metric_{name}'])\n",
    "\n",
    "    # Training Data \n",
    "\n",
    "    fig.add_trace(go.Scatter(x=[*range(1,epochs+1)],\n",
    "                             y=loss_hist[\"train\"],\n",
    "                              line=dict(color=\"#94D4F6\",width=2),\n",
    "                             name='train-loss'),row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=[*range(1,epochs+1)],\n",
    "                             y=metric_hist[\"train\"][name],\n",
    "                             line=dict(color=\"#94D4F6\",width=2),\n",
    "                             name=f'train-{name}'),row=1, col=2)\n",
    "\n",
    "    # Validation Data\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=[*range(1,epochs+1)],\n",
    "                             y=loss_hist[\"val\"],\n",
    "                             line=dict(color=\"#454545\",width=2),\n",
    "                             name='val-loss'),row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=[*range(1,epochs+1)],\n",
    "                             y=metric_hist[\"val\"][name],\n",
    "                             line=dict(color=\"#454545\",width=2),\n",
    "                             name=f'val-{name}'),row=1, col=2)\n",
    "\n",
    "    fig.update_layout(template='plotly_white',\n",
    "                      title='Train / Validation Data Splitting',\n",
    "                      font=dict(family='sans-serif',size=12),\n",
    "                      width=1200)\n",
    "\n",
    "    fig.update_traces({'marker_line_width':3, \n",
    "                       'marker_line_color':\"black\",\n",
    "                       'marker_size':8,\n",
    "                       'opacity':1.0,\n",
    "                       'marker':{'showscale':True,'reversescale':True, 'cmid':0, 'size':10},\n",
    "                      })\n",
    "\n",
    "    fig.update_coloraxes(colorscale=\"tealgrn\")\n",
    "    fig.update_layout(coloraxis_showscale=False)\n",
    "    fig.show()\n",
    "    \n",
    "plot_res(metric_hist,'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "846f83fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:29:27.330464Z",
     "iopub.status.busy": "2022-07-02T11:29:27.329722Z",
     "iopub.status.idle": "2022-07-02T11:29:27.345800Z",
     "shell.execute_reply": "2022-07-02T11:29:27.344680Z"
    },
    "papermill": {
     "duration": 0.031575,
     "end_time": "2022-07-02T11:29:27.348044",
     "exception": false,
     "start_time": "2022-07-02T11:29:27.316469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 0],\n",
       "       [0, 6]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = val_dataset[:10]\n",
    "\n",
    "model.eval()\n",
    "logits = nn_model(X_test.to(device))\n",
    "probs = torch.sigmoid(logits)\n",
    "probs\n",
    "\n",
    "threshold = .5\n",
    "confusion_matrix(y_test[:10], (probs.cpu() >= threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358be869",
   "metadata": {
    "papermill": {
     "duration": 0.012232,
     "end_time": "2022-07-02T11:29:27.373122",
     "exception": false,
     "start_time": "2022-07-02T11:29:27.360890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33.356261,
   "end_time": "2022-07-02T11:29:28.408205",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-02T11:28:55.051944",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
